#!/usr/bin/env python3
"""
Streamlitã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹ç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã€APIå‘¼ã³å‡ºã—ã€ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’ç›£è¦–
"""

import sqlite3
import time
import logging
from datetime import datetime
import pandas as pd
from pathlib import Path
import json

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class AppStateMonitor:
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹ç›£è¦–"""
    
    def __init__(self, db_path='j_stock_v2.db'):
        self.db_path = db_path
        self.monitoring_data = {
            'start_time': datetime.now().isoformat(),
            'database': {},
            'api_calls': {},
            'errors': [],
            'performance': {}
        }
    
    def check_database_state(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®çŠ¶æ…‹ç¢ºèª"""
        logger.info("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯...")
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã‚’ç¢ºèª
            tables = [
                'portfolios',
                'stock_master',
                'financial_metrics',
                'price_history',
                'watchlist',
                'api_cache',
                'app_settings'
            ]
            
            for table in tables:
                try:
                    cursor.execute(f"SELECT COUNT(*) FROM {table}")
                    count = cursor.fetchone()[0]
                    self.monitoring_data['database'][table] = {
                        'record_count': count,
                        'status': 'OK'
                    }
                    logger.info(f"  âœ… {table}: {count}ãƒ¬ã‚³ãƒ¼ãƒ‰")
                except sqlite3.Error as e:
                    self.monitoring_data['database'][table] = {
                        'status': 'ERROR',
                        'error': str(e)
                    }
                    logger.error(f"  âŒ {table}: ã‚¨ãƒ©ãƒ¼ - {e}")
            
            # ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªãƒ‡ãƒ¼ã‚¿ã®è©³ç´°ç¢ºèª
            cursor.execute("""
                SELECT symbol, quantity, average_price, profit_loss_rate_percent
                FROM portfolios
                ORDER BY market_value DESC
                LIMIT 5
            """)
            
            top_holdings = cursor.fetchall()
            if top_holdings:
                logger.info("\nğŸ“ˆ ãƒˆãƒƒãƒ—ä¿æœ‰éŠ˜æŸ„:")
                for symbol, qty, avg_price, pnl in top_holdings:
                    logger.info(f"  - {symbol}: {qty}æ ª @{avg_price:.1f}å†† (æç›Šç‡: {pnl:.1f}%)")
            
            conn.close()
            
        except Exception as e:
            logger.error(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
            self.monitoring_data['errors'].append({
                'type': 'database_connection',
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            })
    
    def check_api_cache(self):
        """APIã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®çŠ¶æ…‹ç¢ºèª"""
        logger.info("\nğŸ”„ APIã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯...")
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®çµ±è¨ˆæƒ…å ±
            cursor.execute("""
                SELECT 
                    api_name,
                    COUNT(*) as cache_count,
                    MIN(cached_at) as oldest_cache,
                    MAX(cached_at) as newest_cache
                FROM api_cache
                GROUP BY api_name
            """)
            
            cache_stats = cursor.fetchall()
            for api, count, oldest, newest in cache_stats:
                self.monitoring_data['api_calls'][api] = {
                    'cache_count': count,
                    'oldest_entry': oldest,
                    'newest_entry': newest
                }
                logger.info(f"  ğŸ“¦ {api}: {count}ä»¶ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥")
                logger.info(f"     æœ€å¤: {oldest}")
                logger.info(f"     æœ€æ–°: {newest}")
            
            # æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç¢ºèª
            cursor.execute("""
                SELECT api_name, COUNT(*) as expired_count
                FROM api_cache
                WHERE datetime(expires_at) < datetime('now')
                GROUP BY api_name
            """)
            
            expired = cursor.fetchall()
            if expired:
                logger.warning("\nâš ï¸ æœŸé™åˆ‡ã‚Œã‚­ãƒ£ãƒƒã‚·ãƒ¥:")
                for api, count in expired:
                    logger.warning(f"  - {api}: {count}ä»¶")
            
            conn.close()
            
        except Exception as e:
            logger.error(f"APIã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
    
    def check_recent_errors(self):
        """æœ€è¿‘ã®ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ç¢ºèª"""
        logger.info("\nğŸš¨ æœ€è¿‘ã®ã‚¨ãƒ©ãƒ¼ç¢ºèª...")
        
        log_files = ['app.log', 'automated_test.log']
        
        for log_file in log_files:
            if Path(log_file).exists():
                with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                    lines = f.readlines()[-50:]  # æœ€å¾Œã®50è¡Œ
                    
                    errors = []
                    for line in lines:
                        if 'ERROR' in line or 'CRITICAL' in line:
                            errors.append(line.strip())
                    
                    if errors:
                        logger.warning(f"\n{log_file}ã®æœ€æ–°ã‚¨ãƒ©ãƒ¼:")
                        for error in errors[-3:]:  # æœ€æ–°3ä»¶
                            logger.warning(f"  - {error[:100]}...")
                            self.monitoring_data['errors'].append({
                                'file': log_file,
                                'message': error,
                                'timestamp': datetime.now().isoformat()
                            })
    
    def check_performance_metrics(self):
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç¢ºèª"""
        logger.info("\nâš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹...")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚ºç¢ºèª
        if Path(self.db_path).exists():
            db_size = Path(self.db_path).stat().st_size / (1024 * 1024)  # MB
            self.monitoring_data['performance']['database_size_mb'] = round(db_size, 2)
            logger.info(f"  ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚º: {db_size:.2f} MB")
        
        # APIå‘¼ã³å‡ºã—é »åº¦ã®è¨ˆç®—
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # éå»1æ™‚é–“ã®APIå‘¼ã³å‡ºã—æ•°
            cursor.execute("""
                SELECT COUNT(*) 
                FROM api_cache 
                WHERE datetime(cached_at) > datetime('now', '-1 hour')
            """)
            
            recent_calls = cursor.fetchone()[0]
            self.monitoring_data['performance']['api_calls_last_hour'] = recent_calls
            logger.info(f"  ğŸ“¡ éå»1æ™‚é–“ã®APIå‘¼ã³å‡ºã—: {recent_calls}å›")
            
            conn.close()
            
        except Exception as e:
            logger.error(f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
    
    def generate_status_report(self):
        """çŠ¶æ…‹ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹ãƒ¬ãƒãƒ¼ãƒˆ")
        logger.info("="*60)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ…‹ã‚µãƒãƒªãƒ¼
        logger.info("\nğŸ—„ï¸ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ…‹:")
        total_records = sum(
            table.get('record_count', 0) 
            for table in self.monitoring_data['database'].values() 
            if isinstance(table, dict) and 'record_count' in table
        )
        logger.info(f"  ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {total_records}")
        
        # ã‚¨ãƒ©ãƒ¼ã‚µãƒãƒªãƒ¼
        error_count = len(self.monitoring_data['errors'])
        if error_count > 0:
            logger.warning(f"\nâš ï¸ æ¤œå‡ºã•ã‚ŒãŸã‚¨ãƒ©ãƒ¼: {error_count}ä»¶")
        else:
            logger.info("\nâœ… ã‚¨ãƒ©ãƒ¼ãªã—")
        
        # æ¨å¥¨äº‹é …
        logger.info("\nğŸ’¡ æ¨å¥¨äº‹é …:")
        
        if error_count > 5:
            logger.info("  1. ã‚¨ãƒ©ãƒ¼ãŒå¤šç™ºã—ã¦ã„ã¾ã™ã€‚ãƒ­ã‚°ã‚’è©³ç´°ã«ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        
        if self.monitoring_data['performance'].get('api_calls_last_hour', 0) > 80:
            logger.info("  2. APIå‘¼ã³å‡ºã—ãŒå¤šã„ã§ã™ã€‚ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚")
        
        if self.monitoring_data['performance'].get('database_size_mb', 0) > 100:
            logger.info("  3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒå¤§ãããªã£ã¦ã„ã¾ã™ã€‚å¤ã„ãƒ‡ãƒ¼ã‚¿ã®æ•´ç†ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_file = f"app_state_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.monitoring_data, f, indent=2, ensure_ascii=False)
        logger.info(f"\nğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: {report_file}")
        
        return self.monitoring_data


def continuous_monitoring(interval=30):
    """ç¶™ç¶šçš„ãªç›£è¦–ãƒ¢ãƒ¼ãƒ‰"""
    logger.info(f"ç¶™ç¶šç›£è¦–ãƒ¢ãƒ¼ãƒ‰é–‹å§‹ï¼ˆ{interval}ç§’é–“éš”ï¼‰")
    logger.info("Ctrl+Cã§çµ‚äº†")
    
    monitor = AppStateMonitor()
    
    try:
        while True:
            logger.info(f"\n{'='*60}")
            logger.info(f"ç›£è¦–å®Ÿè¡Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            logger.info('='*60)
            
            monitor.check_database_state()
            monitor.check_api_cache()
            monitor.check_recent_errors()
            monitor.check_performance_metrics()
            
            # ç°¡æ˜“ã‚µãƒãƒªãƒ¼
            logger.info("\nğŸ“Š ã‚¯ã‚¤ãƒƒã‚¯ã‚µãƒãƒªãƒ¼:")
            
            # ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªä»¶æ•°
            portfolio_count = monitor.monitoring_data['database'].get('portfolios', {}).get('record_count', 0)
            logger.info(f"  - ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ª: {portfolio_count}ä»¶")
            
            # ã‚¨ãƒ©ãƒ¼æ•°
            error_count = len(monitor.monitoring_data['errors'])
            if error_count > 0:
                logger.warning(f"  - ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶ âš ï¸")
            else:
                logger.info("  - ã‚¨ãƒ©ãƒ¼: ãªã— âœ…")
            
            # APIå‘¼ã³å‡ºã—
            api_calls = monitor.monitoring_data['performance'].get('api_calls_last_hour', 0)
            logger.info(f"  - APIå‘¼ã³å‡ºã—ï¼ˆ1æ™‚é–“ï¼‰: {api_calls}å›")
            
            logger.info(f"\næ¬¡å›ãƒã‚§ãƒƒã‚¯: {interval}ç§’å¾Œ...")
            time.sleep(interval)
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰
            monitor.monitoring_data['errors'] = []
            
    except KeyboardInterrupt:
        logger.info("\nç›£è¦–ã‚’çµ‚äº†ã—ã¾ã™...")
        monitor.generate_status_report()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çŠ¶æ…‹ç›£è¦–")
    parser.add_argument('--continuous', action='store_true', help="ç¶™ç¶šç›£è¦–ãƒ¢ãƒ¼ãƒ‰")
    parser.add_argument('--interval', type=int, default=30, help="ç›£è¦–é–“éš”ï¼ˆç§’ï¼‰")
    
    args = parser.parse_args()
    
    if args.continuous:
        continuous_monitoring(args.interval)
    else:
        monitor = AppStateMonitor()
        monitor.check_database_state()
        monitor.check_api_cache()
        monitor.check_recent_errors()
        monitor.check_performance_metrics()
        monitor.generate_status_report()